# **ЗАГРУЗКА ДАННЫХ из API в DWH.**
# **АНАЛИЗ данных с сайта НН.**
[![Typing SVG](https://readme-typing-svg.herokuapp.com?font=Fira+Code&pause=1000&color=4EF752&width=435&lines=%D0%90%D0%9D%D0%90%D0%9B%D0%98%D0%97+;%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85+;%D1%81+%D1%81%D0%B0%D0%B9%D1%82%D0%B0+%D0%9D%D0%9D.)](https://git.io/typing-svg)
# Цель,задача проекта
Провести анализ вакансии по специализации аналитика данных на основе файлов с выгрузкой данных с сайта headhunter, получив ответы на поставленные вопросы.

# Действия в рамках проекта
1 ЭТАП: "Работаю" в VSCode. Загружаю "сырые" данные из REST API из https://transfermarkt-api.vercel.app с помощью Python (библиотека requests). 2 ЭТАП: Запускаю из командной строки docker compose с Minio и с GreenPlum. Сохраняю данные в Minio(в созданном объекте(бакет/корзина) для хранения) в виде json-файлов. Для этого использую Python, библиотеки boto3(библиотека для работы с AWS(S3)-Minio) и json. 3 ЭТАП: При помощи библиотеки pandas(pandas.json_normalize - метод преобразования вложенных json структур в плоский, табличный вид) разворачиваю json-файлы в табличные структуры, которые преобразую в формат parquet, используя библиотеки pyarrow(извлекаем модуль parquet) и io (извлекаем класс BytesIO). Данные в формате parquet с помощью библиотеки boto3 сохраняю в Minio, в тот же бакет, в котором находятся ранее сохраненные json-файлы. 4 ЭТАП: "Работаю" в DBeaver, в GreenPlum. Данные в формате parquet из Minio загружаю во временные таблицы GreenPlum, используя протокол pxf. 5 ЭТАП: Произвожу моделирование данных детального слоя DWH по типу Data Vault2.0, используя инструмент draw.io. 6 ЭТАП: В DWH из временных таблиц GreenPlum загружаю данные в спроектированные объекты по типу Data Vault2.0 с помощью DBT(и пакетом автоматизации automateDV). "Работаю" в VSCode( с DBT), и в DBeaver. 7 ЭТАП: С помощью Аpache Airflow стараюсь объединить все этапы в один общий, непрерывный поток, конвейер данных. "Работаю" в VSCode. Запускаю из командной строки docker compose с Minio, с GreenPlum, c Apache Airflow, c Postgres. Используя Python, пишу DAG для выполнения задач во всем конвейере. Используя веб-интерфейс Airflow через connector подключаю Airflow с Minio, с GreenPlum. "Работаю" в DBeaver, в GreenPlum. Создаю функции, которые будут наполнять временные таблицы из Minio. "Работаю" в веб-интерфейсе Airflow. Запускаю DAG, запускаю конвейер, отслеживаю его работу.
# Выводы по проекту
 Вакансия "аналитика данных" является востребованной; наиболее востребована в г. Москва, в банках и Ozon; предпочтительный режим работы: полный день или удаленная работа; зарплата от 138697 руб. до 179783 руб.
 
 # Используемые навыки и инструменты
 * Python (библиотеки: Pandas).
 * SQL.
 * SQLite.
 * Google Colab.
 * DBeaver
 * Draw.io.

# Статус
- [x] Завершен
