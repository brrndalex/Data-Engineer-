# **ЗАГРУЗКА ДАННЫХ из API в DWH.**
[![Typing SVG](https://readme-typing-svg.herokuapp.com?font=Fira+Code&pause=1000&color=4DF731&width=435&lines=%D0%97%D0%90%D0%93%D0%A0%D0%A3%D0%97%D0%9A%D0%90+%D0%94%D0%90%D0%9D%D0%9D%D0%AB%D0%A5+;%D0%B8%D0%B7+API+%D0%B2+DWH.)](https://git.io/typing-svg)

# Цель,задача проекта
1. В "ручном" режиме: Получить "сырые" данные из API и сохранить их в MinioS3 в форматах json и parquet. Из MinioS3 загрузить данные в формате parquet в DWH, во временные таблицы GreenPlum. И в заключении из временных таблиц загрузить данные в спроектированные объекты Data Vault.
2. С помощью Аpache Airflow объединить все этапы в один общий, непрерывный поток, конвейер данных.

# Действия в рамках проекта
1 ЭТАП: "Работаю" в VSCode. Загружаю "сырые" данные из REST API из https://transfermarkt-api.vercel.app с помощью Python (библиотека requests).  

2 ЭТАП: Запускаю из командной строки docker compose с Minio и с GreenPlum. Сохраняю данные в Minio(в созданном объекте(бакет/корзина) для хранения) в виде json-файлов. Для этого использую Python, библиотеки boto3(библиотека для работы с AWS(S3)-Minio) и json. 

3 ЭТАП: При помощи библиотеки pandas(pandas.json_normalize - метод преобразования вложенных json структур в плоский, табличный вид) разворачиваю json-файлы в табличные структуры, которые преобразую в формат parquet, используя библиотеки pyarrow(извлекаем модуль parquet) и io (извлекаем класс BytesIO). Данные в формате parquet с помощью библиотеки boto3 сохраняю в Minio, в тот же бакет, в котором находятся ранее сохраненные json-файлы. 

4 ЭТАП: "Работаю" в DBeaver, в GreenPlum. Данные в формате parquet из Minio загружаю во временные таблицы GreenPlum, используя протокол pxf.  

5 ЭТАП: Произвожу моделирование данных детального слоя DWH по типу Data Vault2.0, используя инструмент draw.io.  

6 ЭТАП: В DWH из временных таблиц GreenPlum загружаю данные в спроектированные объекты по типу Data Vault2.0 с помощью DBT(и пакетом автоматизации automateDV). "Работаю" в VSCode( с DBT), и в DBeaver.  

7 ЭТАП: С помощью Аpache Airflow стараюсь объединить все этапы в один общий, непрерывный поток, конвейер данных. "Работаю" в VSCode. Запускаю из командной строки docker compose с Minio, с GreenPlum, c Apache Airflow, c Postgres. Используя Python, пишу DAG для выполнения задач во всем конвейере. Используя веб-интерфейс Airflow через connector подключаю Airflow с Minio, с GreenPlum. "Работаю" в DBeaver, в GreenPlum. Создаю функции, которые будут наполнять временные таблицы из Minio. "Работаю" в веб-интерфейсе Airflow. Запускаю DAG, запускаю конвейер, отслеживаю его работу.  

# Выводы по проекту
 УДАЛОСЬ: 1. В "ручном" режиме все получилось, а именно: Получить "сырые" данные из API и сохранить их в MinioS3 в форматах json и parquet. Из MinioS3 загрузить данные в формате parquet в DWH, во временные таблицы GreenPlum. А из временных таблиц загрузить данные в спроектированные объекты по типу Data Vault2.0. 2. Конвейер, получилось только следующее: С помощью Аpache Airflow объединить в один общий, непрерывный поток, конвейер данных, следующие этапы.Получить "сырые" данные из API и сохранить их в MinioS3 в форматах json и parquet. Из MinioS3 загрузить данные в формате parquet в DWH, во временные таблицы GreenPlum. НЕ УДАЛОСЬ: в DWH, из временных таблиц GreenPlum загрузить данные в спроектированные объекты по типу Data Vault2.0(запуск dbt-модели(astronomer cosmor))- с апреля 2024 закрыт доступ к API источника данных.
 
 # Используемые навыки и инструменты
 * Python (библиотеки: Pandas).
 * SQL.
 * SQLite.
 * Google Colab.
 * DBeaver
 * Draw.io.

# Статус
- [x] Завершен
